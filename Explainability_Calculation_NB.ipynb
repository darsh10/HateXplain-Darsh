{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook will generate the Plausibility and Faithfulness results for the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Expainability metrices are based on the work by DeYoung et. al. (2020) [ERASER: A Benchmark to Evaluate Rationalized NLP Models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model choices include 'bert','bert_supervised','birnn','cnngru','birnn_att','birnn_scrat'\n",
    "# attention lambda for bert_supervised is 0.001 and birnn_scrat is 100\n",
    "# for testing_with_lime.py set number of samples is 100 (for faster inference set it lower but the results might not be consistent)\n",
    "\n",
    "\n",
    "\n",
    "#python testing_with_rational.py <model> <att_lambda>\n",
    "!python testing_with_rational.py bert_supervised 0.001\n",
    "#python testing_with_lime.py <model> <number of samples> <att_lambda>\n",
    "!python testing_with_lime.py bert_supervised 100 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>NOTE: Please generate the model explainability output before running this notebook</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import more_itertools as mit\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saha/anaconda3/envs/explain_hate/lib/python3.7/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saha/anaconda3/envs/explain_hate/lib/python3.7/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "# get_annotated_data method is used to load the dataset\n",
    "from Preprocess import *\n",
    "from Preprocess.dataCollect import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data_folder={\n",
    "      '2':{'data_file':'Data/dataset.json','class_label':'Data/classes_two.npy'},\n",
    "      '3':{'data_file':'Data/dataset.json','class_label':'Data/classes.npy'}\n",
    "}\n",
    "\n",
    "# We need to load the dataset with the labels as 'hatespeech', 'offensive', and 'normal' (3-class). \n",
    "\n",
    "params = {}\n",
    "params['num_classes']=3\n",
    "params['data_file']=dict_data_folder[str(params['num_classes'])]['data_file']\n",
    "params['class_names']=dict_data_folder[str(params['num_classes'])]['class_label']\n",
    "\n",
    "data_all_labelled=get_annotated_data(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# The important key here is the 'bert_token'. Set it to True for Bert based models and False for Others.\n",
    "\n",
    "params_data={\n",
    "    'include_special':False,  #True is want to include <url> in place of urls if False will be removed\n",
    "    'bert_tokens':True, #True /False\n",
    "    'type_attention':'softmax', #softmax\n",
    "    'set_decay':0.1,\n",
    "    'majority':2,\n",
    "    'max_length':128,\n",
    "    'variance':10,\n",
    "    'window':4,\n",
    "    'alpha':0.5,\n",
    "    'p_value':0.8,\n",
    "    'method':'additive',\n",
    "    'decay':False,\n",
    "    'normalized':False,\n",
    "    'not_recollect':True,\n",
    "}\n",
    "\n",
    "\n",
    "if(params_data['bert_tokens']):\n",
    "    print('Loading BERT tokenizer...')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=False)\n",
    "else:\n",
    "    print('Loading Normal tokenizer...')\n",
    "    tokenizer=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the whole dataset and get the tokenwise rationales\n",
    "def get_training_data(data):\n",
    "    post_ids_list=[]\n",
    "    text_list=[]\n",
    "    attention_list=[]\n",
    "    label_list=[]\n",
    "    \n",
    "    final_binny_output = []\n",
    "    print('total_data',len(data))\n",
    "    for index,row in tqdm(data.iterrows(),total=len(data)):\n",
    "        annotation=row['final_label']\n",
    "        \n",
    "        text=row['text']\n",
    "        post_id=row['post_id']\n",
    "        annotation_list=[row['label1'],row['label2'],row['label3']]\n",
    "        tokens_all = list(row['text'])\n",
    "#         attention_masks =  [list(row['explain1']),list(row['explain2']),list(row['explain1'])]\n",
    "        \n",
    "        if(annotation!= 'undecided'):\n",
    "            tokens_all,attention_masks=returnMask(row, params_data, tokenizer)\n",
    "            final_binny_output.append([post_id, annotation, tokens_all, attention_masks, annotation_list])\n",
    "\n",
    "    return final_binny_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 22/20148 [00:00<01:34, 212.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_data 20148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20148/20148 [00:59<00:00, 341.00it/s]\n"
     ]
    }
   ],
   "source": [
    "training_data=get_training_data(data_all_labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['18696652_gab',\n",
       " 'offensive',\n",
       " [101,\n",
       "  5310,\n",
       "  25805,\n",
       "  5582,\n",
       "  4319,\n",
       "  2224,\n",
       "  2025,\n",
       "  11382,\n",
       "  3489,\n",
       "  2012,\n",
       "  2035,\n",
       "  4283,\n",
       "  2005,\n",
       "  2008,\n",
       "  19380,\n",
       "  102],\n",
       " [[0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0]],\n",
       " ['hatespeech', 'offensive', 'offensive']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/2154249/identify-groups-of-continuous-numbers-in-a-list\n",
    "def find_ranges(iterable):\n",
    "    \"\"\"Yield range of consecutive numbers.\"\"\"\n",
    "    for group in mit.consecutive_groups(iterable):\n",
    "        group = list(group)\n",
    "        if len(group) == 1:\n",
    "            yield group[0]\n",
    "        else:\n",
    "            yield group[0], group[-1]\n",
    "            \n",
    "# Convert dataset into ERASER format: https://github.com/jayded/eraserbenchmark/blob/master/rationale_benchmark/utils.py\n",
    "def get_evidence(post_id, anno_text, explanations):\n",
    "    output = []\n",
    "\n",
    "    indexes = sorted([i for i, each in enumerate(explanations) if each==1])\n",
    "    span_list = list(find_ranges(indexes))\n",
    "\n",
    "    for each in span_list:\n",
    "        if type(each)== int:\n",
    "            start = each\n",
    "            end = each+1\n",
    "        elif len(each) == 2:\n",
    "            start = each[0]\n",
    "            end = each[1]+1\n",
    "        else:\n",
    "            print('error')\n",
    "\n",
    "        output.append({\"docid\":post_id, \n",
    "              \"end_sentence\": -1, \n",
    "              \"end_token\": end, \n",
    "              \"start_sentence\": -1, \n",
    "              \"start_token\": start, \n",
    "              \"text\": ' '.join([str(x) for x in anno_text[start:end]])})\n",
    "    return output\n",
    "\n",
    "# To use the metrices defined in ERASER, we will have to convert the dataset\n",
    "def convert_to_eraser_format(dataset, method, save_split, save_path, id_division):  \n",
    "    final_output = []\n",
    "    \n",
    "    if save_split:\n",
    "        train_fp = open(save_path+'train.jsonl', 'w')\n",
    "        val_fp = open(save_path+'val.jsonl', 'w')\n",
    "        test_fp = open(save_path+'test.jsonl', 'w')\n",
    "            \n",
    "    for tcount, eachrow in enumerate(dataset):\n",
    "        \n",
    "        temp = {}\n",
    "        post_id = eachrow[0]\n",
    "        post_class = eachrow[1]\n",
    "        anno_text_list = eachrow[2]\n",
    "        majority_label = eachrow[1]\n",
    "        \n",
    "        if majority_label=='normal':\n",
    "            continue\n",
    "        \n",
    "        all_labels = eachrow[4]\n",
    "        explanations = []\n",
    "        for each_explain in eachrow[3]:\n",
    "            explanations.append(list(each_explain))\n",
    "        \n",
    "        # For this work, we have considered the union of explanations. Other options could be explored as well.\n",
    "        if method == 'union':\n",
    "            final_explanation = [any(each) for each in zip(*explanations)]\n",
    "            final_explanation = [int(each) for each in final_explanation]\n",
    "        \n",
    "            \n",
    "        temp['annotation_id'] = post_id\n",
    "        temp['classification'] = post_class\n",
    "        temp['evidences'] = [get_evidence(post_id, list(anno_text_list), final_explanation)]\n",
    "        temp['query'] = \"What is the class?\"\n",
    "        temp['query_type'] = None\n",
    "        final_output.append(temp)\n",
    "        \n",
    "        if save_split:\n",
    "            if not os.path.exists(save_path+'docs'):\n",
    "                os.makedirs(save_path+'docs')\n",
    "            \n",
    "            with open(save_path+'docs/'+post_id, 'w') as fp:\n",
    "                fp.write(' '.join([str(x) for x in list(anno_text_list)]))\n",
    "            \n",
    "            if post_id in id_division['train']:\n",
    "                train_fp.write(json.dumps(temp)+'\\n')\n",
    "            \n",
    "            elif post_id in id_division['val']:\n",
    "                val_fp.write(json.dumps(temp)+'\\n')\n",
    "            \n",
    "            elif post_id in id_division['test']:\n",
    "                test_fp.write(json.dumps(temp)+'\\n')\n",
    "            else:\n",
    "                print(post_id)\n",
    "    \n",
    "    if save_split:\n",
    "        train_fp.close()\n",
    "        val_fp.close()\n",
    "        test_fp.close()\n",
    "        \n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The post_id_divisions file stores the train, val, test split ids. We select only the test ids.\n",
    "with open('./Data/post_id_divisions.json') as fp:\n",
    "    id_division = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Data/Evaluation/Model_Eval/train.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d504033a2a6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msave_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./Data/Evaluation/Model_Eval/'\u001b[0m  \u001b[0;31m#The dataset in Eraser Format will be stored here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mconvert_to_eraser_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_division\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-bd076273f658>\u001b[0m in \u001b[0;36mconvert_to_eraser_format\u001b[0;34m(dataset, method, save_split, save_path, id_division)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_split\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mtrain_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'train.jsonl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mval_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'val.jsonl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mtest_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'test.jsonl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Data/Evaluation/Model_Eval/train.jsonl'"
     ]
    }
   ],
   "source": [
    "method = 'union'\n",
    "save_split = True\n",
    "save_path = './Data/Evaluation/Model_Eval/'  #The dataset in Eraser Format will be stored here.\n",
    "convert_to_eraser_format(training_data, method, save_split, save_path, id_division)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd eraserbenchmark-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--data_dir : Location of the folder which contains the dataset in eraser format\n",
    "#--results : The location of the model output file in eraser format\n",
    "#--score_file : The file name and location to write the output\n",
    "\n",
    "!PYTHONPATH=./:$PYTHONPATH python rationale_benchmark/metrics.py --split test --strict --data_dir ../Data/Evaluation/Model_Eval --results ../explanations_dicts/bestModel_bert_base_uncased_Attn_train_TRUE_0.001_explanation_top5.json --score_file ../model_explain_output.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the required results\n",
    "with open('../model_explain_output.json') as fp:\n",
    "    output_data = json.load(fp)\n",
    "\n",
    "print('\\nPlausibility')\n",
    "print('IOU F1 :', output_data['iou_scores'][0]['macro']['f1'])\n",
    "print('Token F1 :', output_data['token_prf']['instance_macro']['f1'])\n",
    "print('AUPRC :', output_data['token_soft_metrics']['auprc'])\n",
    "\n",
    "print('\\nFaithfulness')\n",
    "print('Comprehensiveness :', output_data['classification_scores']['comprehensiveness'])\n",
    "print('Sufficiency', output_data['classification_scores']['sufficiency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:explain_hate]",
   "language": "python",
   "name": "conda-env-explain_hate-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
